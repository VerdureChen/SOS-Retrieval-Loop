# Codebase of "Spiral of Silence: How is Large Language Model Killing Information Retrieval?‚ÄîA Case Study on Open Domain Question Answering"
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#news-and-updates">News and Updates</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#evaluation">Evaluation</a></li>
    <li><a href="#more-use-examples">More Use Examples</a></li> 
  </ol>
</details>

<!-- News and Updates -->

## News and Updates
- [06/04/2024] üìù You can refer to our [Arxiv paper](https://arxiv.org/pdf/2404.10496.pdf) and [Zhihu (Chinese)](https://zhuanlan.zhihu.com/p/701436734) for more details.
- [05/15/2024] üéâ Our paper has been accepted to ACL 2024.
- [05/12/2024] üíª Published code used in our experiments.



<!-- Introduction -->

## Introduction

In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems. 

![Pipeline Structure](pipeline.png)



### What does our code currently provide?
<!--
1. Êñπ‰æø‰ΩøÁî®ÁöÑËø≠‰ª£Ê®°ÊãüÂ∑•ÂÖ∑ÔºöÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™Êòì‰∫é‰ΩøÁî®ÁöÑËø≠‰ª£Ê®°ÊãüÂ∑•ÂÖ∑ÔºåÈÄöËøáËûçÂêàElasticSearch„ÄÅLangChainÂíåapi-for-llmÁöÑÂäüËÉΩÔºåËÉΩÂ§üÊñπ‰æøÂú∞Âä†ËΩΩÊï∞ÊçÆÈõÜÔºåÈÄâÊã©ÂêÑÁßçLLMÂíåÊ£ÄÁ¥¢ÊéíÂ∫èÊ®°ÂûãÂíåËá™Âä®Ëø≠‰ª£Ê®°Êãü„ÄÇ
2. ÊîØÊåÅÂ§ö‰∏™Êï∞ÊçÆÈõÜÔºöÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éNatural Questions, TriviaQA, WebQuestions, PopQAÔºåÈÄöËøáÂ∞ÜÊï∞ÊçÆËΩ¨Âåñ‰∏∫jsonlÊ†ºÂºèÔºå‰Ω†ÂèØ‰ª•‰ΩøÁî®Êú¨ÊñáÁöÑÊ°ÜÊû∂ÂØπ‰ªª‰ΩïÊï∞ÊçÆËøõË°åÂÆûÈ™å„ÄÇ
3. ÊîØÊåÅÂ§öÁßçÊ£ÄÁ¥¢Ê®°ÂûãÂíåÈáçÊéíÂ∫èÊ®°ÂûãÔºöBM25, ContrieverÔºåLLM-EmbedderÔºåBGEÔºåUPRÔºåMonoT5Á≠â„ÄÇ
4. ÊîØÊåÅ‰ΩøÁî®Â∏∏Áî®ÁöÑLLMÁîüÊàêÔºögpt-3.5-turbo, chatglm3-6b, qwen-14b-chat, llama2-13b-chat, baichuan2-13b-chatÁ≠â„ÄÇ
5. ÊîØÊåÅÂ§öÁßçRAG pipelineÊºîÂèòËØÑ‰º∞ÊñπÊ≥ïÔºåÂØπÊØèÊ¨°ÂÆûÈ™åÁöÑÂ§ßÈáèÁªìÊûúËøõË°åËá™Âä®ËØÑ‰ª∑Êï¥ÁêÜ„ÄÇ
-->

1. **User-friendly Iteration Simulation Tool:** We offer an easy-to-use iteration simulation tool that integrates functionalities from [ElasticSearch](https://www.elastic.co/elasticsearch/), [LangChain](https://github.com/langchain-ai/langchain/), and [api-for-open-llm](https://github.com/xusenlinzy/api-for-open-llm), allowing for convenient dataset loading, selection of various LLMs and retrieval-ranking models, and automated iterative simulation.
2. **Support for Multiple Datasets:** Including but not limited to [Natural Questions](https://github.com/google-research-datasets/natural-questions), [TriviaQA](https://github.com/mandarjoshi90/triviaqa), [WebQuestions](https://github.com/brmson/dataset-factoid-webquestions), [PopQA](https://github.com/AlexTMallen/adaptive-retrieval). By converting data to jsonl format, you can use the framework in this paper to experiment with any data.
3. **Support for Various Retrieval and Re-ranking Models:** [BM25](https://python.langchain.com/docs/integrations/retrievers/elastic_search_bm25), [Contriever](https://github.com/facebookresearch/contriever), [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding), [BGE](https://github.com/FlagOpen/FlagEmbedding), [UPR](https://github.com/DevSinghSachan/unsupervised-passage-reranking), [MonoT5](https://github.com/castorini/pygaggle) and more.
4. **Support for frequently-used LLMs:** [GPT-3.5 turbo](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates), [chatglm3-6b](https://huggingface.co/THUDM/chatglm3-6b), [Qwen-14B-Chat](https://huggingface.co/Qwen/Qwen-14B-Chat), [Llama-2-13b-chat-hf](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf), [Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat).
5. **Supports Various RAG Pipeline Evolution Evaluation Methods:** Automatically organizes and assesses the vast amount of results from each experiment.


<!-- GETTING STARTED -->

## Installation
<!-- 
Êàë‰ª¨ÁöÑÊ°ÜÊû∂‰æùËµñElasticSearch 8.11.1Âíåapi-for-open-llmÔºåÂõ†Ê≠§ÈúÄË¶ÅÂÖàÂÆâË£ÖËøô‰∏§‰∏™Â∑•ÂÖ∑„ÄÇÊàë‰ª¨Âª∫ËÆÆÊÇ®‰∏ãËΩΩÁõ∏ÂêåÁâàÊú¨ÁöÑ[ElasticSearch 8.11.1](https://www.elastic.co/guide/en/elasticsearch/reference/8.11/targz.html)ÔºåÂπ∂‰∏îÂú®ÂêØÂä®Ââç‰ªéÂÖ∂config/elasticsearch.ymlÊñá‰ª∂‰∏≠ËÆæÁΩÆÂ•ΩÂØπÂ∫îÁöÑhttp.portÂíåhttp.hostÔºåÂÆÉ‰ª¨Â∞ÜÁî®‰∫éÊú¨‰ªìÂ∫ì‰ª£Á†ÅËøêË°åÁöÑÈÖçÁΩÆ„ÄÇ
ÂΩìÊÇ®ÂÆâË£Öapi-for-open-llmÊó∂ÔºåËØ∑Ê†πÊçÆÂÖ∂[ÂÆòÊñπ‰ªìÂ∫ì](https://github.com/xusenlinzy/api-for-open-llm/blob/master/docs/SCRIPT.md)ÁöÑÊåáÂºïÔºåÂÆâË£ÖÂ•ΩËøêË°åÊÇ®ÊâÄÈúÄÊ®°ÂûãÁöÑ‰æùËµñÂíåÁéØÂ¢É„ÄÇÊÇ®Âú®.envÊñá‰ª∂‰∏≠ËÆæÁΩÆÁöÑPORT‰πüÂ∞Ü‰Ωú‰∏∫Êú¨‰ª£Á†ÅÂ∫ìÈúÄË¶ÅÁöÑÈÖçÁΩÆ„ÄÇ
-->
Our framework depends on **ElasticSearch 8.11.1** and **api-for-open-llm**, therefore it is necessary to install these two tools first. We suggest downloading the same version of [ElasticSearch 8.11.1](https://www.elastic.co/guide/en/elasticsearch/reference/8.11/targz.html), and before starting, set the appropriate `http.port` and `http.host` in the `config/elasticsearch.yml` file, as these will be used for the configuration needed to run the code in this repository.

When installing api-for-open-llm, please follow the instructions provided by its [repository](https://github.com/xusenlinzy/api-for-open-llm/blob/master/docs/SCRIPT.md) to install all the dependencies and environment required to run the model you need. The `PORT` you configure in the `.env` file will also serve as a required configuration for this codebase.

### Install via GitHub

First, clone the repo:
```sh
git clone --recurse-submodules git@github.com:VerdureChen/SOS-Retrieval-Loop.git
```

Then, 

```sh
cd SOS-Retrieval-Loop
```

To install the required packages, you can create a conda environment:

```sh
conda create --name SOS_LOOP python=3.10
```

Activate the conda environment:

```sh
conda activate SOS_LOOP
```

then use pip to install required packages:

```sh
pip install -r requirements.txt
```

## Usage

Please see [Installation](#installation) to install the required packages.
<!-- 
Âú®ËøêË°åÊàë‰ª¨ÁöÑÊ°ÜÊû∂‰πãÂâçÔºåÊÇ®ÈúÄË¶ÅÂÖàÂêØÂä®ElasticSearchÂíåapi-for-open-llm„ÄÇÂú®ÂêØÂä®ElasticSearchÊó∂ÔºåÊÇ®ÈúÄË¶ÅÂú®ÂÖ∂config/elasticsearch.ymlÊñá‰ª∂‰∏≠ËÆæÁΩÆÂ•ΩÂØπÂ∫îÁöÑhttp.portÂíåhttp.hostÔºåÂÆÉ‰ª¨Â∞ÜÁî®‰∫éÊú¨‰ªìÂ∫ì‰ª£Á†ÅËøêË°åÁöÑÈÖçÁΩÆ„ÄÇ
Âú®ÂêØÂä®api-for-open-llmÊó∂ÔºåÊÇ®ÈúÄË¶ÅÂú®.envÊñá‰ª∂‰∏≠ËÆæÁΩÆÂ•ΩPORTÔºåÂÆÉ‰πüÂ∞Ü‰Ωú‰∏∫Êú¨‰ª£Á†ÅÂ∫ìÈúÄË¶ÅÁöÑÈÖçÁΩÆ„ÄÇ
-->
Before running our framework, you need to start **ElasticSearch** and **api-for-open-llm**. When starting **ElasticSearch**, you need to set the appropriate `http.port` and `http.host` in the `config/elasticsearch.yml` file, as these will be used for the configuration needed to run the code in this repository.

When starting **api-for-open-llm**, you need to set the `PORT` in the `.env` file, which will also serve as a required configuration for this codebase.

<!--
### Configuration
Áî±‰∫éÊàë‰ª¨ÁöÑ‰ª£Á†ÅÊ∂âÂèäÂà∞ËæÉÂ§öÁöÑÊï∞ÊçÆÈõÜ„ÄÅÊ®°ÂûãÂíåÁ¥¢ÂºïÁ≠âÂäüËÉΩÔºåÊàë‰ª¨‰ΩøÁî®‰∏âÁ∫ßconfigÊñπÂºèÊù•ÊéßÂà∂‰ª£Á†ÅÁöÑËøêË°åÔºö
1. Âú®‰ª£Á†ÅÁöÑÁâπÂÆöÂäüËÉΩÔºàÂ¶ÇÊ£ÄÁ¥¢„ÄÅÈáçÊéí„ÄÅÁîüÊàêÁ≠âÔºâÁõÆÂΩï‰∏ãÁöÑconfigÊñá‰ª∂Â§πÂÜÖÔºåÂ≠òÊîæÁùÄËØ•ÂäüËÉΩÁöÑÈÖçÁΩÆÊñá‰ª∂Ê®°ÊùøÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∫ÜÊâÄÊúâÂèØËá™Ë°åÈÖçÁΩÆÁöÑÂèÇÊï∞Ôºå‰Ω†ÂèØ‰ª•‰ªéÈÖçÁΩÆÊñá‰ª∂ÁöÑÊñá‰ª∂Âêç‰∏≠Á°ÆËÆ§ËØ•ÈÖçÁΩÆÊñá‰ª∂ÂØπÂ∫îÂäüËÉΩÂíåÊï∞ÊçÆÈõÜ„ÄÇ‰æãÂ¶ÇÔºå`src/retrieval_loop/retrieve_configs/bge-base-config-nq.json`ÊòØ‰∏Ä‰∏™Ê£ÄÁ¥¢ÈÖçÁΩÆÊñá‰ª∂ÔºåÂØπÂ∫îÁöÑÊï∞ÊçÆÈõÜÊòØNatural QuestionsÔºå‰ΩøÁî®ÁöÑÊ®°ÂûãÊòØBGE-base„ÄÇÊàë‰ª¨‰ª•`src/retrieval_loop/index_configs/bge-base-config-psgs_w100.json`‰∏∫‰æãÔºö
```json
{
      "new_text_file": "../../data_v2/input_data/DPR/psgs_w100.jsonl", 
      "retrieval_model": "bge-base",
      "index_name": "bge-base_faiss_index",
      "index_path": "../../data_v2/indexes",
      "index_add_path": "../../data_v2/indexes",
      "page_content_column": "contents",
      "index_exists": false,
      "normalize_embeddings": true,
      "query_files": ["../../data_v2/input_data/DPR/nq-test-h10.jsonl"],
      "query_page_content_column": "question",
      "output_files": ["../../data_v2/ret_output/DPR/nq-test-h10-bge-base"],
      "elasticsearch_url": "http://xxx.xxx.xxx.xxx:xxx"
}
```
ÂÖ∂‰∏≠Ôºå`new_text_file`ÊòØÈúÄË¶ÅÊñ∞Ê∑ªÂä†Âà∞Á¥¢ÂºïÁöÑÊñáÊ°£Ë∑ØÂæÑÔºå`retrieval_model`ÊòØ‰ΩøÁî®ÁöÑÊ£ÄÁ¥¢Ê®°ÂûãÔºå`index_name`ÊòØÁ¥¢ÂºïÁöÑÂêçÁß∞Ôºå`index_path`ÊòØÁ¥¢ÂºïÁöÑÂ≠òÂÇ®Ë∑ØÂæÑÔºå`index_add_path`ÊòØÁ¥¢ÂºïÁöÑÂ¢ûÈáèÊñáÊ°£Âú®Á¥¢Âºï‰∏≠ÁöÑIDÂ≠òÊîæË∑ØÂæÑÔºàËøôÂØπ‰∫éÊàë‰ª¨ÈúÄË¶Å‰ªéÁ¥¢Âºï‰∏≠Âà†Èô§ÁâπÂÆöÊñáÊ°£ÁâπÂà´ÊúâÁî®ÔºâÔºå`page_content_column`ÊòØÊñáÊ°£Êñá‰ª∂‰∏≠ÂæÖÁ¥¢ÂºïÁöÑÊñáÊú¨ÁöÑÂàóÂêçÔºå`index_exists`ÊåáÁ§∫Á¥¢ÂºïÊòØÂê¶Â∑≤ÁªèÂ≠òÂú®ÔºàÂ¶ÇÊûúËÆæÁΩÆ‰∏∫falseÂàô‰ºöÊñ∞Âª∫Áõ∏Â∫îÁ¥¢ÂºïÔºåÂê¶Âàô‰ªéË∑ØÂæÑËØªÂèñÂ∑≤Â≠òÂú®ÁöÑÁ¥¢ÂºïÔºâÔºå`normalize_embeddings`ÊòØÊòØÂê¶ÂØπÊ£ÄÁ¥¢Ê®°ÂûãÁöÑËæìÂá∫ËøõË°åÂΩí‰∏ÄÂåñÔºå`query_files`ÊòØÊü•ËØ¢Êñá‰ª∂ÁöÑË∑ØÂæÑÔºå`query_page_content_column`ÊòØÊü•ËØ¢Êñá‰ª∂‰∏≠Êü•ËØ¢ÊñáÊú¨ÁöÑÂàóÂêçÔºå`output_files`ÊòØËæìÂá∫Ê£ÄÁ¥¢ÁªìÊûúÊñá‰ª∂ÁöÑË∑ØÂæÑÔºà‰∏équery_files‰∏Ä‰∏ÄÂØπÂ∫îÔºâÔºå`elasticsearch_url`ÊòØElasticSearchÁöÑurl„ÄÇ
2. Áî±‰∫éÊàë‰ª¨ÈÄöÂ∏∏ÈúÄË¶ÅÂú®‰∏Ä‰∏™pipeline‰∏≠ËûçÂêàÂ§ö‰∏™Ê≠•È™§Ôºå‰∏éÁ§∫‰æãËÑöÊú¨`src/run_loop.sh`Áõ∏ÈÄÇÂ∫îÁöÑÔºåÊàë‰ª¨Êèê‰æõ‰∏Ä‰∏™ÂÖ®Â±ÄÈÖçÁΩÆÊñá‰ª∂`src/test_function/test_configs/template_total_config.json`ÔºåÂú®Ëøô‰∏™ÂÖ®Â±ÄÈÖçÁΩÆÊñá‰ª∂‰∏≠Ôºå‰Ω†ÂèØ‰ª•‰∏ÄÊ¨°ÊÄßÂØπÊØè‰∏™Èò∂ÊÆµÁöÑÂèÇÊï∞ËøõË°åÈÖçÁΩÆÔºå‰Ω†‰∏çÂøÖÂú®ÂÖ∂‰∏≠ÈÖçÁΩÆÊâÄÊúâÂèÇÊï∞ÔºåÂè™ÈúÄÈÖçÁΩÆ‰Ω†Áõ∏ÂØπ‰∫éÊ®°ÊùøÈÖçÁΩÆÊñá‰ª∂ÈúÄË¶Å‰øÆÊîπÁöÑÂèÇÊï∞Âç≥ÂèØ„ÄÇ
3. ‰∏∫‰∫ÜÊèêÂçáËÑöÊú¨ËøêË°åÊïàÁéáÔºå`src/run_loop.sh`ËÑöÊú¨ÊîØÊåÅÂØπ‰∫éÊüê‰∏™Ê£ÄÁ¥¢-ÈáçÊéíÊñπÊ≥ïÔºåÂêåÊó∂ËøêË°åÂ§ö‰∏™Êï∞ÊçÆÈõÜÂíåLLMÁîüÊàêÁöÑÁªìÊûúÔºå‰∏∫‰∫ÜËÉΩÂ§üÊõ¥ÁÅµÊ¥ªÂú∞ÈÖçÁΩÆÊ≠§Á±ªÂÆûÈ™åÔºåÊàë‰ª¨ÊîØÊåÅÂú®`src/run_loop.sh`ÂÜÖÈÉ®ÈÄöËøá`rewrite_configs.py`Âú®pipelineËøêË°åËøáÁ®ã‰∏≠ÁîüÊàêÊñ∞ÁöÑÈÖçÁΩÆÊñá‰ª∂„ÄÇ‰æãÂ¶ÇÔºåÂΩìÊàë‰ª¨ÈúÄË¶ÅÂæ™ÁéØËøêË°åpipelineÊó∂ÔºåÊàë‰ª¨ÈúÄË¶ÅËÆ∞ÂΩïÊØè‰∏ÄËΩÆÁöÑconfigÂÜÖÂÆπÔºåÂú®ÊØèÊ¨°ËøõË°åÊ£ÄÁ¥¢‰πãÂâçÔºåËÑöÊú¨Â∞ÜËøêË°åÔºö
```bash
python ../rewrite_configs.py --total_config "${USER_CONFIG_PATH}" \
                          --method "${RETRIEVAL_MODEL_NAME}" \
                          --data_name "nq" \
                          --loop "${LOOP_NUM}" \
                          --stage "retrieval" \
                          --output_dir "${CONFIG_PATH}" \
                          --overrides '{"query_files": ['"${QUERY_FILE_LIST}"'], "output_files": ['"${OUTPUT_FILE_LIST}"'] , "elasticsearch_url": "'"${elasticsearch_url}"'", "normalize_embeddings": false}'
```
ÂÖ∂‰∏≠Ôºå`--total_config`ÊòØÂÖ®Â±ÄÈÖçÁΩÆÊñá‰ª∂ÁöÑË∑ØÂæÑÔºå`--method`ÊòØÊ£ÄÁ¥¢ÊñπÊ≥ïÁöÑÂêçÁß∞Ôºå`--data_name`ÊòØÊï∞ÊçÆÈõÜÁöÑÂêçÁß∞Ôºå`--loop`ÊòØÂΩìÂâçÂæ™ÁéØÁöÑËΩÆÊï∞Ôºå`--stage`ÊòØÂΩìÂâçpipelineÁöÑÈò∂ÊÆµÔºå`--output_dir`ÊòØÊñ∞ÁîüÊàêÁöÑÈÖçÁΩÆÊñá‰ª∂ÁöÑÂ≠òÂÇ®Ë∑ØÂæÑÔºå`--overrides`ÊòØÈúÄË¶Å‰øÆÊîπÁöÑÂèÇÊï∞ÔºàÂêåÊ†∑ÊòØÊØè‰∏™‰ªªÂä°configÊ®°ÊùøÁöÑÂ≠êÈõÜÔºâ„ÄÇ
4. Âú®ËøõË°åÈÖçÁΩÆÊó∂‰Ω†ÈúÄË¶ÅÊ≥®ÊÑè‰∏âÁ±ªÈÖçÁΩÆÁöÑ‰ºòÂÖàÁ∫ßÔºöÁ¨¨‰∏ÄÁ∫ßÊòØÊØè‰∏™‰ªªÂä°configÊ®°Êùø‰∏≠ÁöÑÈªòËÆ§ÈÖçÁΩÆÔºåÁ¨¨‰∫åÁ∫ßÊòØÂÖ®Â±ÄÈÖçÁΩÆÊñá‰ª∂ÔºåÁ¨¨‰∏âÁ∫ßÊòØpipelineËøêË°åËøáÁ®ã‰∏≠ÁîüÊàêÁöÑÊñ∞ÈÖçÁΩÆÊñá‰ª∂„ÄÇÂú®pipelineËøêË°åËøáÁ®ã‰∏≠ÔºåÁ¨¨‰∫åÁ∫ßÈÖçÁΩÆ‰ºöË¶ÜÁõñÁ¨¨‰∏ÄÁ∫ßÈÖçÁΩÆÔºåÁ¨¨‰∏âÁ∫ßÈÖçÁΩÆ‰ºöË¶ÜÁõñÁ¨¨‰∫åÁ∫ßÈÖçÁΩÆ„ÄÇ
-->

### Configuration
Since our code involves many datasets, models, and index functionalities, we use a three-level config method to control the operation of the code:
1. In the config folder of the specific function (such as retrieval, re-ranking, generation, etc.), there is a configuration file template for that function, which contains all the parameters that can be configured by yourself. You can confirm the function and dataset corresponding to the configuration file from the file name of the configuration file. For example, `src/retrieval_loop/retrieve_configs/bge-base-config-nq.json` is a retrieval configuration file, corresponding to the Natural Questions dataset, and using the BGE-base model. We take `src/retrieval_loop/index_configs/bge-base-config-psgs_w100.json` as an example:
    
   ```json
    {
          "new_text_file": "../../data_v2/input_data/DPR/psgs_w100.jsonl", 
          "retrieval_model": "bge-base",
          "index_name": "bge-base_faiss_index",
          "index_path": "../../data_v2/indexes",
          "index_add_path": "../../data_v2/indexes",
          "page_content_column": "contents",
          "index_exists": false,
          "normalize_embeddings": true,
          "query_files": ["../../data_v2/input_data/DPR/nq-test-h10.jsonl"],
          "query_page_content_column": "question",
          "output_files": ["../../data_v2/ret_output/DPR/nq-test-h10-bge-base"],
          "elasticsearch_url": "http://xxx.xxx.xxx.xxx:xxx"
    }
    ```
   
    Where `new_text_file` is the path to the document to be newly added to the index, `retrieval_model` is the retrieval model used, `index_name` is the name of the index, `index_path` is the storage path of the index, `index_add_path` is the path where the ID of the incremental document in the index is stored (this is particularly useful when we need to delete specific documents from the index), `page_content_column` is the column name of the text to be indexed in the document file, `index_exists` indicates whether the index already exists (if set to false, the corresponding index will be created, otherwise the existing index will be read from the path), `normalize_embeddings` is whether to normalize the output of the retrieval model, `query_files` is the path to the query file, `query_page_content_column` is the column name of the query text in the query file, `output_files` is the path to the output retrieval result file (corresponding to query_files), and `elasticsearch_url` is the url of ElasticSearch.

2. Since we usually need to integrate multiple steps in a pipeline, corresponding to the example script `src/run_loop.sh`, we provide a global configuration file `src/test_function/test_configs/template_total_config.json`. In this global configuration file, you can configure the parameters of each stage at once. You do not need to configure all the parameters in it, just the parameters you need to modify relative to the template configuration file.

3. In order to improve the efficiency of script running, the `src/run_loop.sh` script supports running multiple datasets and LLM-generated results for a retrieval-re-ranking method at the same time. To flexibly configure such experiments, we support generating new configuration files during the pipeline run in `src/run_loop.sh` through `rewrite_configs.py`. For example, when we need to run the pipeline in a loop, we need to record the config content of each round. Before each retrieval, the script will run:
    
   ```bash
    python ../rewrite_configs.py --total_config "${USER_CONFIG_PATH}" \
                              --method "${RETRIEVAL_MODEL_NAME}" \
                              --data_name "nq" \
                              --loop "${LOOP_NUM}" \
                              --stage "retrieval" \
                              --output_dir "${CONFIG_PATH}" \
                              --overrides '{"query_files": ['"${QUERY_FILE_LIST}"'], "output_files": ['"${OUTPUT_FILE_LIST}"'] , "elasticsearch_url": "'"${elasticsearch_url}"'", "normalize_embeddings": false}'
    ```
   
    Where `--total_config` is the path to the global configuration file, `--method` is the name of the retrieval method, `--data_name` is the name of the dataset, `--loop` is the number of the current loop, `--stage` is the stage of the current pipeline, `--output_dir` is the storage path of the newly generated configuration file, and `--overrides` is the parameters that need to be modified (also a subset of the template configuration file for each task).

4. When configuring, you need to pay attention to the priority of the three types of configurations: the first level is the default configuration in each task config template, the second level is the global configuration file, and the third level is the new configuration file generated during the pipeline run. During the pipeline run, the second level configuration will override the first level configuration, and the third level configuration will override the second level configuration.


<!--
### Running the Code
ÈÄöËøá‰ª•‰∏ãÊ≠•È™§Ôºå‰Ω†ÂèØ‰ª•Â§çÁé∞Êàë‰ª¨ÁöÑÂÆûÈ™åÔºåÂú®Ê≠§‰πãÂâçËØ∑ÈòÖËØª[Configuration](#configuration)ÈÉ®ÂàÜ‰∫ÜËß£ÈÖçÁΩÆÊñá‰ª∂ÁöÑËÆæÁΩÆÔºö
1. Êï∞ÊçÆÈõÜÈ¢ÑÂ§ÑÁêÜÔºö‰∏çËÆ∫ÊòØÊü•ËØ¢ËøòÊòØÊñáÊ°£ÔºåÊàë‰ª¨ÈÉΩÈúÄË¶ÅÂ∞ÜÊï∞ÊçÆÈõÜËΩ¨Âåñ‰∏∫jsonlÊ†ºÂºè„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠‰ΩøÁî®data.wikipedia_split.psgs_w100Êï∞ÊçÆÔºåÂèØÂèÇËÄÉ[DPR‰ªìÂ∫ì](https://github.com/facebookresearch/DPR?tab=readme-ov-file#resources--data-formats)ÁöÑËØ¥ÊòéÂ∞ÜÂÖ∂‰∏ãËΩΩËá≥`data_v2/raw_data/DPR`ÁõÆÂΩï‰∏ãÂπ∂Ëß£Âéã„ÄÇÊàë‰ª¨Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑËÑöÊú¨`data_v2/gen_dpr_hc_jsonl.py`ÔºåÂèØ‰ª•Â∞ÜÊï∞ÊçÆÈõÜËΩ¨Âåñ‰∏∫jsonlÊ†ºÂºèÂπ∂ÊîæÁΩÆ‰∫é`data_v2/input_data/DPR`„ÄÇÂÆûÈ™å‰∏≠‰ΩøÁî®Âà∞ÁöÑqueryÊñá‰ª∂‰Ωç‰∫é`data_v2/input_data/DPR/sampled_query`„ÄÇ
   ```bash
    cd data_v2
    python gen_dpr_hc_jsonl.py 
    ```
2. ÁîüÊàêZero-Shot RAGÁªìÊûúÔºö‰ΩøÁî®`src/llm_zero_generate/run_generate.sh`ÔºåÈÄöËøá‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑÈÖçÁΩÆÔºåËÉΩÂ§üÊâπÈáèÂåñÁîüÊàêÊâÄÊúâÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑzero-shot RAGÁªìÊûú„ÄÇÂú®ËÑöÊú¨ÂºÄÂßã‰ΩçÁΩÆÈÖçÁΩÆÂ¶Ç‰∏ãÂèÇÊï∞Ôºö
    ```bash
    MODEL_NAMES=(chatglm3-6b) #chatglm3-6b qwen-14b-chat llama2-13b-chat baichuan2-13b-chat gpt-3.5-turbo
    GENERATE_BASE_AND_KEY=(
       "gpt-3.5-turbo http://XX.XX.XX.XX:XX/v1 xxx"
       "chatglm3-6b http://XX.XX.XX.XX:XX/v1 xxx"
       "qwen-14b-chat http://XX.XX.XX.XX:XX/v1 xxx"
       "llama2-13b-chat http://XX.XX.XX.XX:XX/v1 xxx"
       "baichuan2-13b-chat http://XX.XX.XX.XX:XX/v1 xxx"
      )
    
    DATA_NAMES=(tqa pop nq webq)
    CONTEXT_REF_NUM=1
    QUESTION_FILE_NAMES=(
      "-test-sample-200.jsonl"
      "-upr_rerank_based_on_bm25.json"
    )
    LOOP_CONFIG_PATH_NAME="../run_configs/original_retrieval_config"
    
    TOTAL_LOG_DIR="../run_logs/original_retrieval_log"
    QUESTION_FILE_PATH_TOTAL="../../data_v2/loop_output/DPR/original_retrieval_result"
    TOTAL_OUTPUT_DIR="../../data_v2/loop_output/DPR/original_retrieval_result"
    ```
    ÂÖ∂‰∏≠Ôºå`MODEL_NAMES`ÊòØÈúÄË¶ÅÁîüÊàêÁªìÊûúÁöÑÊ®°ÂûãÂêçÁß∞ÂàóË°®Ôºå`GENERATE_BASE_AND_KEY`Áî±Ê®°ÂûãÂêçÁß∞„ÄÅapiÂú∞ÂùÄÂíåkeyÁªÑÊàêÔºå`DATA_NAMES`ÊòØÊï∞ÊçÆÈõÜÂêçÁß∞ÂàóË°®Ôºå`CONTEXT_REF_NUM`ÊòØ‰∏ä‰∏ãÊñáÂèÇËÄÉÊï∞ÈáèÔºàzero-shotÊÉÖÂÜµ‰∏ãËÆæÁΩÆ‰∏∫0ÔºâÔºå`QUESTION_FILE_NAMES`ÊòØÊü•ËØ¢Êñá‰ª∂ÂêçÁß∞ÂàóË°®Ôºà‰ΩÜÈúÄË¶ÅÊ≥®ÊÑèÔºåËÑöÊú¨ÈÄöËøáÊñá‰ª∂ÂêçÁöÑÂâçÁºÄËØÜÂà´ÂÖ∂ÊâÄÂ±ûÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§Ë¶ÅÊü•ËØ¢nq-test-sample-200.jsonlÔºå‰Ω†ÈúÄË¶ÅÂú®`DATA_NAMES`ÂåÖÂê´`nq`ÔºåËÄåÊú¨Â≠óÊÆµÂàô‰ªÖÂ°´ÂÜô`-test-sample-200.jsonl`ÔºâÔºå`LOOP_CONFIG_PATH_NAME`Âíå`TOTAL_LOG_DIR`ÂàÜÂà´ÊòØËøêË°å‰∏≠configÂíåÊó•ÂøóÁöÑÂ≠òÂÇ®Ë∑ØÂæÑÔºå`QUESTION_FILE_PATH_TOTAL`ÊòØÊü•ËØ¢Êñá‰ª∂Â≠òÂÇ®Ë∑ØÂæÑÔºå`TOTAL_OUTPUT_DIR`ÊòØÁîüÊàêÁªìÊûúÂ≠òÂÇ®Ë∑ØÂæÑ„ÄÇ
    ÈÖçÁΩÆÂ•ΩÂêéÔºåËøêË°åËÑöÊú¨Ôºö
    ```bash
    cd src/llm_zero_generate
    bash run_generate.sh
    ```
3. Âª∫Á´ãÊï∞ÊçÆÈõÜÁ¥¢ÂºïÔºö‰ΩøÁî®`src/retrieval_loop/run_index_builder.sh`ÔºåÈÄöËøá‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ`MODEL_NAMES`Âíå`DATA_NAMES`ÈÖçÁΩÆÔºåËÉΩÂ§ü‰∏ÄÊ¨°ÊÄßÂª∫Á´ãÊâÄÊúâÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑÁ¥¢Âºï„ÄÇ‰Ω†‰πüÂèØ‰ª•ÈÄöËøáÈÖçÁΩÆ`query_files`Âíå`output_files`Êù•Ëé∑ÂæóÂü∫‰∫éËØ•Á¥¢ÂºïÁöÑÂØπÂ∫îÊñπÊ≥ïÁöÑÊ£ÄÁ¥¢ÁªìÊûú„ÄÇÂú®Êàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠ÊâÄÊúâÊ£ÄÁ¥¢Ê®°ÂûãcheckpointÊîæÂú®`ret_model`ÁõÆÂΩï‰∏ã„ÄÇ
   ËøêË°åÔºö
   ```bash
    cd src/retrieval_loop
    bash run_index_builder.sh
   ```
4. ÂØπZero-Shot RAGÁîüÊàêÁöÑÊï∞ÊçÆËøõË°åÂêéÂ§ÑÁêÜÔºåÂØπÁîüÊàêÁöÑÊñáÊú¨ËøõË°åËøáÊª§„ÄÅIDÈáçÂëΩÂêçÁ≠âÊìç‰ΩúÔºö‰ΩøÁî®`src/post_process/post_process.sh`ÔºåÈÄöËøá‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ`MODEL_NAMES`Âíå`QUERY_DATA_NAMES`ÈÖçÁΩÆÔºåËÉΩÂ§üÂú®‰∏ÄÊ¨°ËøêË°å‰∏≠Â∞ÜÊâÄÊúâÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑzero-shot RAGÁîüÊàêÁªìÊûú„ÄÇÂØπ‰∫éZero-shotÊï∞ÊçÆÔºåÊàë‰ª¨Â∞Ü`LOOP_NUM`ËÆæÁΩÆ‰∏∫0Ôºå`LOOP_CONFIG_PATH_NAME`,`TOTAL_LOG_DIR`,`TOTAL_OUTPUT_DIR`ÊåáÂÆöËÑöÊú¨configs,logsÂíåoutputË∑ØÂæÑÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®Â∞ÜËá™Âä®ÁîüÊàê„ÄÇ`FROM_METHOD`Ë°®Á§∫ÂΩìÂâçÂæÖÂ§ÑÁêÜÁöÑÊñáÊú¨ÁöÑÁîüÊàêÊñπÊ≥ïÔºåËøôÂ∞Ü‰Ωú‰∏∫tagÊ∑ªÂä†Âà∞Â§ÑÁêÜÂêéÁöÑÊñáÊ°£ID‰∏≠„ÄÇ`INPUT_FILE_PATH`‰∏∫ÂæÖÂ§ÑÁêÜÁöÑÊñáÊú¨Êñá‰ª∂Ë∑ØÂæÑÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂåÖÂê´ÊØè‰∏™Êï∞ÊçÆÈõÜ‰∏∫ÂêçÁß∞ÁöÑÁõÆÂΩïÔºåÁõÆÂΩï‰∏ãÊòØÂêÑ‰∏™Zero-shotÁªìÊûúÊñá‰ª∂„ÄÇÊ≠§Â§ñËøòÈúÄÊ≥®ÊÑèÔºåÊ†∏ÂØπ`INPUT_FILE_NAME`‰∏éÂÆûÈôÖËæìÂÖ•ÊñáÊú¨Âêç‰∏ÄËá¥„ÄÇ
    ËøêË°åÔºö
    ```bash
    cd src/post_process
    bash post_process.sh
    ```
5. Â∞ÜZero-Shot RAGÁîüÊàêÁöÑÂÜÖÂÆπÊ∑ªÂä†Âà∞Á¥¢ÂºïÔºåÂπ∂Ëé∑ÂæóÂä†ÂÖ•Zero-ShotÊï∞ÊçÆÂêéÁöÑÊ£ÄÁ¥¢ÁªìÊûúÔºö‰ΩøÁî®`src/run_zero-shot.sh`ÔºåÈÄöËøá‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ`GENERATE_MODEL_NAMES`Âíå`QUERY_DATA_NAMES`ÈÖçÁΩÆÔºåËÉΩÂ§üÂú®‰∏ÄÊ¨°ËøêË°å‰∏≠Â∞ÜÊâÄÊúâÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑzero-shot RAGÁîüÊàêÁªìÊûúÊ∑ªÂä†Âà∞Á¥¢ÂºïÔºåÂπ∂Ëé∑ÂæóÂä†ÂÖ•Zero-ShotÊï∞ÊçÆÂêéÁöÑÊ£ÄÁ¥¢ÁªìÊûú„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØ`run_items`ÂàóË°®Ë°®Á§∫‰∫ÜÈúÄË¶ÅËøêË°åÁöÑÊ£ÄÁ¥¢-ÈáçÊéíÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂÖÉÁ¥†ÊûÑÊàê‰æãÂ¶Ç`"item6 bm25 monot5"`ÔºåË°®ÊòéÊú¨Ê¨°ËøêË°åÁöÑÁ¨¨ÂÖ≠‰∏™Zero-Shot RAGÂÆûÈ™åÊòØÂü∫‰∫éBM25+MonoT5ÁöÑÊ£ÄÁ¥¢-ÈáçÊéíÊñπÊ≥ï„ÄÇ
   ËøêË°åÔºö
   ```bash
   cd src
   bash run_zero-shot.sh
    ```
6. ËøêË°å‰∏ªË¶ÅÁöÑLLM-generated TextÊ®°ÊãüÂæ™ÁéØÔºö‰ΩøÁî®`src/run_loop.sh`ÔºåÈÄöËøá‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ`GENERATE_MODEL_NAMES`Âíå`QUERY_DATA_NAMES`ÈÖçÁΩÆÔºåËÉΩÂ§üÊâπÈáèÂåñËøêË°åÊâÄÊúâÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑLLM-generated TextÊ®°ÊãüÂæ™ÁéØ„ÄÇ‰Ω†ÂèØ‰ª•ÈÄöËøáËÆæÁΩÆ`TOTAL_LOOP_NUM`Êù•ÊéßÂà∂Âæ™ÁéØÊ¨°Êï∞ÔºåÁî±‰∫éÊ∂âÂèäÂà∞Á¥¢ÂºïÂ§öÊ¨°Êõ¥Êñ∞ÔºåÊØèÊ¨°Âè™ËÉΩËøêË°å‰∏Ä‰∏™Ê£ÄÁ¥¢-ÈáçÊéíÊñπÊ≥ïpipeline„ÄÇÂ¶ÇÊûú‰Ω†ÊÉ≥ÊîπÂèòRAG pipeline‰∏≠LLMËÉΩÁúãÂà∞ÁöÑ‰∏ä‰∏ãÊñá‰∏™Êï∞Ôºå‰Ω†ÂèØ‰ª•ÈÄöËøá‰øÆÊîπ`CONTEXT_REF_NUM`Êù•ÂÆûÁé∞ÔºåÈªòËÆ§ËÆæÁΩÆ‰∏∫5„ÄÇ
    ËøêË°åÔºö
    ```bash
    cd src
    bash run_loop.sh
    ```
-->

### Running the Code
Through the following steps, you can reproduce our experiments. Before that, please read the [Configuration](#configuration) section to understand the settings of the configuration file:
1. Dataset Preprocessing: Whether it is a query or a document, we need to convert the dataset to jsonl format. In our experiments, we use the data.wikipedia_split.psgs_w100 dataset, which can be downloaded to the `data_v2/raw_data/DPR` directory and unzipped according to the instructions in the [DPR repository](https://github.com/facebookresearch/DPR?tab=readme-ov-file#resources--data-formats). We provide a simple script `data_v2/gen_dpr_hc_jsonl.py`, which can convert the dataset to jsonl format and place it in `data_v2/input_data/DPR`. The query files used in the experiment are located in `data_v2/input_data/DPR/sampled_query`.
   ```bash
    cd data_v2
    python gen_dpr_hc_jsonl.py 
    ```
2. Generate Zero-Shot RAG Results: Use `src/llm_zero_generate/run_generate.sh`, by modifying the configuration in the file, you can generate zero-shot RAG results for all data and models in batches. Configure the following parameters at the beginning of the script:
    ```bash
    MODEL_NAMES=(chatglm3-6b) #chatglm3-6b qwen-14b-chat llama2-13b-chat baichuan2-13b-chat gpt-3.5-turbo
    GENERATE_BASE_AND_KEY=(
       "gpt-3.5-turbo http://XX.XX.XX.XX:XX/v1 xxx"
       "chatglm3-6b http://XX.XX.XX.XX:XX/v1 xxx"
       "qwen-14b-chat http://XX.XX.XX.XX:XX/v1 xxx"
       "llama2-13b-chat http://XX.XX.XX.XX:XX/v1 xxx"
       "baichuan2-13b-chat http://XX.XX.XX.XX:XX/v1 xxx"
      )
    DATA_NAMES=(tqa pop nq webq)
    CONTEXT_REF_NUM=1
    QUESTION_FILE_NAMES=(
      "-test-sample-200.jsonl"
      "-upr_rerank_based_on_bm25.json"
    )
    LOOP_CONFIG_PATH_NAME="../run_configs/original_retrieval_config"
    
    TOTAL_LOG_DIR="../run_logs/original_retrieval_log"
    QUESTION_FILE_PATH_TOTAL="../../data_v2/loop_output/DPR/original_retrieval_result"
    TOTAL_OUTPUT_DIR="../../data_v2/loop_output/DPR/original_retrieval_result"
    ```
    Where `MODEL_NAMES` is a list of model names for which results need to be generated, `GENERATE_BASE_AND_KEY` consists of the model name, api address, and key, `DATA_NAMES` is a list of dataset names, `CONTEXT_REF_NUM` is the number of context references (set to 0 in the zero-shot case), `QUESTION_FILE_NAMES` is a list of query file names (but note that the script identifies the dataset to which it belongs by the prefix of the file name, so to query nq-test-sample-200.jsonl, you need to include `nq` in `DATA_NAMES`, and this field only fills in `-test-sample-200.jsonl`), `LOOP_CONFIG_PATH_NAME` and `TOTAL_LOG_DIR` are the storage paths of the running config and logs, `QUESTION_FILE_PATH_TOTAL` is the query file storage path, and `TOTAL_OUTPUT_DIR` is the storage path of the generated results.
    After configuring, run the script:
    ```bash
    cd src/llm_zero_generate
    bash run_generate.sh
    ```
3. Build Dataset Index: Use `src/retrieval_loop/run_index_builder.sh`, by modifying the `MODEL_NAMES` and `DATA_NAMES` configuration in the file, you can build indexes for all data and models at once. You can also obtain retrieval results based on the corresponding method by configuring `query_files` and `output_files`. In our experiments, all retrieval model checkpoints are placed in the `ret_model` directory.
   Run:
   ```bash
    cd src/retrieval_loop
    bash run_index_builder.sh
   ```
4. Post-process the data generated by Zero-Shot RAG, filter the generated text, rename IDs, etc.: Use `src/post_process/post_process.sh`, by modifying the `MODEL_NAMES` and `QUERY_DATA_NAMES` configuration in the file, you can process all data and models of zero-shot RAG generation results in one run. For Zero-shot data, we set `LOOP_NUM` to 0, and `LOOP_CONFIG_PATH_NAME`, `TOTAL_LOG_DIR`, `TOTAL_OUTPUT_DIR` specify the paths of the script configs, logs, and output, respectively. `FROM_METHOD` indicates the generation method of the current text to be processed, which will be added as a tag to the processed document ID. `INPUT_FILE_PATH` is the path to the text file to be processed, with each directory containing the name of each dataset, and each directory containing various Zero-shot result files. In addition, make sure that `INPUT_FILE_NAME` is consistent with the actual input text name.
    Run:
    ```bash
    cd src/post_process
    bash post_process.sh
    ```
5. Add the content generated by Zero-Shot RAG to the index and obtain the retrieval results after adding Zero-Shot data: Use `src/run_zero-shot.sh`, by modifying the `GENERATE_MODEL_NAMES` and `QUERY_DATA_NAMES` configuration in the file, you can add the zero-shot RAG generation results of all data and models to the index in one run, and obtain the retrieval results after adding Zero-Shot data. Note that the `run_items` list indicates the retrieval-re-ranking methods that need to be run, where each element is constructed as `"item6 bm25 monot5"`, indicating that the sixth Zero-Shot RAG experiment in this run is based on the BM25+MonoT5 retrieval-re-ranking method.
   Run:
   ```bash
   cd src
   bash run_zero-shot.sh
    ```
6. Run the main LLM-generated Text simulation loop: Use `src/run_loop.sh`, by modifying the `GENERATE_MODEL_NAMES` and `QUERY_DATA_NAMES` configuration in the file, you can run the LLM-generated Text Simulation loop for all data and models in batches. You can control the number of loops by setting `TOTAL_LOOP_NUM`. Since it involves updating the index multiple times, only one retrieval-re-ranking method pipeline can be run at a time. If you want to change the number of contexts that LLM can see in the RAG pipeline, you can do so by modifying `CONTEXT_REF_NUM`, which is set to 5 by default.
   Run:
    ```bash
    cd src
    bash run_loop.sh
    ```
<!--
## Evaluation
Èù¢ÂêëÂÆûÈ™å‰∏≠ÁîüÊàêÁöÑÂ§ßÈáèÊï∞ÊçÆÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÊîØÊåÅÂ§öÁßçÊâπÈáèÂåñÁöÑËØÑ‰º∞ÊñπÊ≥ïÔºåËÆæÁΩÆ`src/evaluation/run_context_eva.sh`ËÑöÊú¨‰∏≠ÁöÑ`QUERY_DATA_NAMES`Âíå`RESULT_NAMES`ÂêéÔºå‰Ω†ÂèØ‰ª•ÈÄâÊã©‰ªªÊÑèÂèØÊîØÊåÅÁöÑtaskËøõË°åËØÑ‰º∞ÔºåÂåÖÊã¨Ôºö
1. `TASK="retrieval"`ÔºöÂØπÊØèÊ¨°Ëø≠‰ª£ÁöÑÊ£ÄÁ¥¢ÂíåÈáçÊéíÂ∫èÁªìÊûúËøõË°åËØÑ‰º∞ÔºåÂåÖÊã¨Acc@5ÂíåAcc@20„ÄÇ
2. `TASK="QA"`ÔºöÂØπÊØèÊ¨°Ëø≠‰ª£ÁöÑQAÁªìÊûúËøõË°åËØÑ‰º∞ÔºåÊåáÊ†á‰∏∫EM„ÄÇ
3. `TASK="context_answer"`ÔºöËÆ°ÁÆóÊØè‰∏™LLMÂú®ÊØèÊ¨°Ëø≠‰ª£ÁªìÊùüÊó∂ÂõûÁ≠îÊ≠£Á°ÆÔºàEM=1ÔºâÊàñÈîôËØØÔºàEM=0ÔºâÊó∂‰∏ä‰∏ãÊñáÔºàÈªòËÆ§‰∏∫top5Ê£ÄÁ¥¢ÁªìÊûúÔºâ‰∏≠ÂåÖÂê´Ê≠£Á°ÆÁ≠îÊ°àÁöÑÊñáÊ°£Êï∞Èáè„ÄÇ
4. `TASK="bleu"`ÔºöËÆ°ÁÆóÊØèÊ¨°Ëø≠‰ª£‰∏ä‰∏ãÊñáÁöÑSELF-BLEUÂÄºÔºåÈªòËÆ§ËÆ°ÁÆó2-gramÂíå3-gram„ÄÇ
5. `TASK="percentage"`ÔºöËÆ°ÁÆóÊØèÊ¨°Ëø≠‰ª£top5„ÄÅ20„ÄÅ50ÁöÑ‰∏ä‰∏ãÊñá‰∏≠ÂêÑLLMÂíå‰∫∫Á±ªÁîüÊàêÊñáÊú¨ÁöÑÁôæÂàÜÊØî„ÄÇ
6. `TASK="misQA"`ÔºöÂú®MisinformationÂÆûÈ™å‰∏≠ÔºåËÆ°ÁÆóÊØèÊ¨°Ëø≠‰ª£ÁöÑQAÁªìÊûú‰∏≠ÁâπÂÆöÈîôËØØÁ≠îÊ°àÁöÑEM„ÄÇ
7. `TASK="QA_llm_mis"`Âíå`TASK="QA_llm_right"`ÔºöÂú®MisinformationÂÆûÈ™å‰∏≠ÔºåËÆ°ÁÆóÊØèÊ¨°Ëø≠‰ª£ÁöÑQAÁªìÊûú‰∏≠ÁâπÂÆöÈîôËØØÁ≠îÊ°àÊàñÊ≠£Á°ÆÁ≠îÊ°àÁªèËøáGPT-3.5-TurboÂà§Âà´ÂêéÁ°ÆÂÆöÊñáÊú¨Á°ÆÂÆûÊîØÊåÅËØ•Á≠îÊ°àÁöÑÊÉÖÂÜµÔºàÂèÇËÄÉÊñá‰∏≠ÁöÑEM_llmÔºâ„ÄÇ
8. `TASK="filter_bleu_*"`Âíå`TASK="filter_source_*"`ÔºöÂú®FilteringÂÆûÈ™å‰∏≠Ôºå‰∏çÂêåËøáÊª§ÊñπÊ≥ï‰∏ãËÆ°ÁÆóÊØèÊ¨°Ëø≠‰ª£ÁöÑËØÑ‰º∞ÁªìÊûúÔºå*‰ª£Ë°®ÂâçÈù¢Â∑≤ÁªèÂá∫Áé∞ÁöÑËØÑ‰º∞ÂÜÖÂÆπÔºàretireval, percentage, context_answerÔºâ„ÄÇ
ËØÑ‰º∞ÂêéÁîüÊàêÁöÑÁªìÊûúÊñá‰ª∂ÈªòËÆ§Â≠ò‰∫éÂØπÂ∫î`RESULT_DIR/RESULT_NAME/QUERY_DATA_NAME/results`ÁõÆÂΩï‰∏ã„ÄÇ
-->
## Evaluation
For the large amount of results generated in the experiment, our framework supports various batch evaluation methods. After setting `QUERY_DATA_NAMES` and `RESULT_NAMES` in `src/evaluation/run_context_eva.sh`, you can choose any supported task for evaluation, including:
1. `TASK="retrieval"`: Evaluate the retrieval and re-ranking results of each iteration, including Acc@5 and Acc@20.
2. `TASK="QA"`: Evaluate the QA results of each iteration (EM).
3. `TASK="context_answer"`: Calculate the number of documents in the contexts (default top 5 retrieval results) that contain the correct answer when each LLM answers correctly (EM=1) or incorrectly (EM=0) at the end of each iteration.
4. `TASK="bleu"`: Calculate the SELF-BLEU value of the contexts (default top 5 retrieval results) at each iteration, with 2-gram and 3-gram calculated by default.
5. `TASK="percentage"`: Calculate the percentage of each LLM and human-generated text in the top 5, 20, and 50 contexts at each iteration.
6. `TASK="misQA"`: In the Misinformation experiment, calculate the EM of specific incorrect answers in the QA results at each iteration.
7. `TASK="QA_llm_mis"` and `TASK="QA_llm_right"`: In the Misinformation experiment, calculate the situation where specific incorrect or correct answers in the QA results at each iteration are determined to be supported by the text after being judged by GPT-3.5-Turbo (refer to EM_llm in the paper).
8. `TASK="filter_bleu_*"` and `TASK="filter_source_*"`: In the Filtering experiment, calculate the evaluation results of each iteration under different filtering methods, where * represents the evaluation content that has already appeared (retrieval, percentage, context_answer).

The results generated after evaluation are stored by default in the corresponding `RESULT_DIR/RESULT_NAME/QUERY_DATA_NAME/results` directory.


<!--
## More Use Examples
### Âú®‰∏çÂêåËø≠‰ª£Èò∂ÊÆµ‰ΩøÁî®‰∏çÂêåÁöÑLLM
ÈÄöËøá‰øÆÊîπ`src/run_loop.sh`‰∏≠ÁöÑ`GENERATE_MODEL_NAMES`ÈÖçÁΩÆÔºå‰Ω†ÂèØ‰ª•Âú®‰∏çÂêåÈò∂ÊÆµ‰ΩøÁî®‰∏çÂêåÁöÑLLMÔºå‰æãÂ¶ÇÔºö
```bash
GENERATE_MODEL_NAMES_F3=(qwen-0.5b-chat qwen-1.8b-chat qwen-4b-chat)
GENERATE_MODEL_NAMES_F7=(qwen-7b-chat llama2-7b-chat baichuan2-7b-chat)
GENERATE_MODEL_NAMES_F10=(gpt-3.5-turbo qwen-14b-chat llama2-13b-chat)
```
Ë°®Á§∫Âú®Ââç‰∏âËΩÆËø≠‰ª£‰∏≠‰ΩøÁî®qwen-0.5b-chat„ÄÅqwen-1.8b-chatÂíåqwen-4b-chatÔºåÁ¨¨ÂõõÂà∞‰∏ÉËΩÆËø≠‰ª£‰∏≠‰ΩøÁî®qwen-7b-chat„ÄÅllama2-7b-chatÂíåbaichuan2-7b-chatÔºåÁ¨¨ÂÖ´Âà∞ÂçÅËΩÆËø≠‰ª£‰∏≠‰ΩøÁî®gpt-3.5-turbo„ÄÅqwen-14b-chatÂíållama2-13b-chat„ÄÇÊàë‰ª¨ÂèØ‰ª•‰ª•Ê≠§Êù•Ê®°ÊãüLLMÊÄßËÉΩÈöèÊó∂Èó¥ÂèòÂåñËÄåÂ¢ûÂº∫Êó∂ÂØπRAGÁ≥ªÁªüÁöÑÂΩ±Âìç„ÄÇËØ∑ËÆ∞ÂæóÂú®`GENERATE_BASE_AND_KEY`‰∏≠Êèê‰æõÂØπÂ∫îÁöÑ‰ø°ÊÅØÔºåÂêåÊó∂‰øÆÊîπ`GENERATE_TASK`‰∏∫`update_generate`„ÄÇ
### MisinformationÂÆûÈ™å
1. Êàë‰ª¨È¶ñÂÖà‰ΩøÁî®GPT-3.5-Turbo‰∏∫ÊØè‰∏™queryÁîüÊàê5‰∏™ÈîôËØØÁ≠îÊ°à,ÁÑ∂ÂêéÈöèÊú∫ÈÄâÂèñ‰∏Ä‰∏™ÈîôËØØÁ≠îÊ°àÔºå‰ª§ÊâÄÊúâÂÆûÈ™å‰ΩøÁî®Âà∞ÁöÑLLM‰∏∫ÂÖ∂ÁîüÊàêÊîØÊåÅÊñáÊú¨„ÄÇ‰Ω†ÂèØ‰ª•‰øÆÊîπ`src/misinfo/mis_config`ÁõÆÂΩï‰∏ãÁöÑ`{dataset}_mis_config_answer_gpt.json`ÈÖçÁΩÆÊñá‰ª∂ÔºàÁî®‰∫éÁîüÊàêÈîôËØØÁ≠îÊ°àÔºâÂíå`{dataset}_mis_config_passage_{llm}.json`ÈÖçÁΩÆÊñá‰ª∂ÔºàÁî®‰∫éÁîüÊàêÂåÖÂê´ÈîôËØØÁ≠îÊ°àÁöÑÊñáÊú¨ÔºâÔºåÂØπÊØè‰∏™Êï∞ÊçÆÈõÜÂíåÁîüÊàêÂÜÖÂÆπË∑ØÂæÑ‰ª•Âèäapi‰ø°ÊÅØËøõË°åÈÖçÁΩÆ.
2. ËøêË°å`src/misinfo/run_gen_misinfo.sh`ËÑöÊú¨ÔºåËÉΩÂ§üÂú®‰∏ÄÊ¨°ËøêË°å‰∏≠ÁîüÊàêÊâÄÊúâÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑMisinformationÊñáÊú¨„ÄÇ
3. ÂèÇËÄÉ[Running the Code](#running-the-code)‰∏≠ÁöÑÊ≠•È™§4Âà∞6ÔºåÂ∞ÜÁîüÊàêÁöÑMisinformationÊñáÊú¨Ê∑ªÂä†Âà∞Á¥¢ÂºïÔºåÂπ∂Ëé∑ÂæóÂä†ÂÖ•MisinformationÊï∞ÊçÆÂêéÁöÑRAG„ÄÇ
4. ÂèÇËÄÉ[Evaluation](#evaluation)ËøêË°å`src/evaluation/run_context_eva.sh`ËÑöÊú¨ÔºåÂØπMisinformationÂÆûÈ™åÁöÑÁªìÊûúËøõË°åËØÑ‰º∞ÔºåÊé®Ëçê‰ΩøÁî®`"retrieval"`,`"context_answer"`,`"QA_llm_mis"`,`"QA_llm_right"`ËØÑ‰º∞‰ªªÂä°„ÄÇ
### FilteringÂÆûÈ™å
Âú®Êàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨ÂàÜÂà´ËøõË°å‰∫ÜÂØπÊ£ÄÁ¥¢Ëé∑ÂæóÁöÑ‰∏ä‰∏ãÊñáÁöÑSELF-BLEUËøáÊª§ÂíåÊù•Ê∫êËøáÊª§„ÄÇ
1. SELF-BLEUËøáÊª§ÔºöÊàë‰ª¨ËÆ°ÁÆótopÊ£ÄÁ¥¢ÁªìÊûúÁöÑSELF-BLEUÂÄºÔºåÁ°Æ‰øùËæìÂÖ•ÁªôLLMÁöÑ‰∏ä‰∏ãÊñáÁª¥ÊåÅËæÉÈ´òÁöÑÁõ∏‰ººÂ∫¶Ôºà‰πüÂ∞±ÊòØËæÉ‰ΩéÁöÑSELF-BLEUÂÄºÔºåÈªòËÆ§‰∏∫0.4Ôºâ„ÄÇË¶ÅÂêØÁî®Ê≠§ÂäüËÉΩÔºåÂ∞Ü`src/run_loop.sh`‰∏≠ËÆæÁΩÆ`FILTER_METHOD_NAME=filter_bleu`„ÄÇ
2. Êù•Ê∫êËøáÊª§ÔºöÊàë‰ª¨ËØÜÂà´topÊ£ÄÁ¥¢ÁªìÊûú‰∏≠Êù•Ê∫ê‰∫éLLMÁöÑÊñáÊú¨Âπ∂Â∞ÜÂÖ∂ÊéíÈô§„ÄÇÊàë‰ª¨‰ΩøÁî®[Hello-SimpleAI/chatgpt-qa-detector-roberta](https://huggingface.co/Hello-SimpleAI/chatgpt-qa-detector-roberta)ËøõË°åËØÜÂà´ÔºåËØ∑Âú®ÂÆûÈ™åÂºÄÂßãÂâçÂ∞ÜËØ•checkpointÊîæÁΩÆ‰∫é`ret_model`ÁõÆÂΩï‰∏ã„ÄÇË¶ÅÂêØÁî®Ê≠§ÂäüËÉΩÔºåÂ∞Ü`src/run_loop.sh`‰∏≠ËÆæÁΩÆ`FILTER_METHOD_NAME=filter_source`„ÄÇ
3. ÂèÇËÄÉ[Evaluation](#evaluation)ËøêË°å`src/evaluation/run_context_eva.sh`ËÑöÊú¨ÔºåÂØπFilteringÂÆûÈ™åÁöÑÁªìÊûúËøõË°åËØÑ‰º∞ÔºåÊé®Ëçê‰ΩøÁî®`"retrieval"`,`"QA"`,`"filter_bleu_*"`,`"filter_source_*"`ËØÑ‰º∞‰ªªÂä°„ÄÇ
### ‰ªéÁ¥¢ÂºïÂà†Èô§Áõ∏Â∫îÊñáÊ°£
Áî±‰∫éÊàë‰ª¨ÁöÑÂÆûÈ™åÊ∂âÂèäÂà∞Á¥¢ÂºïÁöÑÂä®ÊÄÅÊõ¥Êñ∞ÔºåÊàë‰ª¨‰∏çÂèØËÉΩÂú®ÊØèÊ¨°Ê®°Êãü‰∏≠ÈáçÊñ∞‰ªéÈõ∂ÊûÑÈÄ†Á¥¢Âºï„ÄÇÁõ∏ÂèçÂú∞ÔºåÂú®ÊØèÊ¨°Ê®°ÊãüÊó∂ÔºåÊàë‰ª¨ÈÉΩ‰ºöÂ∞ÜÊñ∞Â¢ûÊñáÊú¨IDËÆ∞ÂΩïÂú®`src/run_logs`‰∏≠ÂØπÂ∫îÊ≠§Ê¨°ÂÆûÈ™åÁöÑ`index_add_logs`ÁõÆÂΩï‰∏ãÔºåÂæÖÂÆûÈ™åÁªìÊùüÂêéÔºåÊàë‰ª¨ÈÄöËøá`src/post_process/delete_doc_from_index.py`ËÑöÊú¨Âà†Èô§Á¥¢Âºï‰∏≠ÁöÑÁõ∏Â∫îÊñáÊ°£„ÄÇ
1. ÂΩìÈúÄË¶ÅÂà†Èô§BM25Á¥¢Âºï‰∏≠ÁöÑÊñáÊ°£Êó∂ÔºåËøêË°åÔºö
```bash
cd src/post_process
python delete_doc_from_index.py --config_file_path delete_configs/delete_config_bm25.json
```
ÂÖ∂‰∏≠Ôºå`src/post_process/delete_configs/delete_config_bm25.json`ÊòØÂØπÂ∫îÈÖçÁΩÆÊñá‰ª∂ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂèÇÊï∞Ôºö
```json
{
    "id_files": ["../run_logs/zero-shot_retrieval_log_low/index_add_logs"],
    "model_name": "bm25",
    "index_path": "../../data_v2/indexes",
    "index_name": "bm25_psgs_index",
    "elasticsearch_url": "http://xxx.xxx.xxx.xxx:xxx",
    "delete_log_path": "../run_logs/zero-shot_retrieval_log_low/index_add_logs"
}
```
ÂÖ∂‰∏≠Ôºå`id_files`ÊòØÈúÄË¶ÅÂà†Èô§ÁöÑÊñáÊ°£IDÊâÄÂú®Êñá‰ª∂ÁõÆÂΩïÔºå`model_name`ÊòØÁ¥¢ÂºïÊ®°ÂûãÂêçÁß∞Ôºå`index_path`ÊòØÁ¥¢ÂºïÂ≠òÂÇ®Ë∑ØÂæÑÔºå`index_name`ÊòØÁ¥¢ÂºïÂêçÁß∞Ôºå`elasticsearch_url`ÊòØElasticSearchÁöÑurlÔºå`delete_log_path`ÊòØÂà†Èô§ÊñáÊ°£IDËÆ∞ÂΩïÁöÑÂ≠òÂÇ®Ë∑ØÂæÑ„ÄÇ‰∏çÂêåÁ¥¢ÂºïÁöÑIDÊñá‰ª∂ÂèØ‰ª•Ê∑∑ÂêàÊîæÁΩÆÂú®Âêå‰∏™ÁõÆÂΩï‰∏ãÔºåËÑöÊú¨Â∞ÜËá™Âä®ËØªÂèñÁõÆÂΩï‰∏ãÂØπÂ∫îËØ•Á¥¢ÂºïÁöÑÊñáÊ°£IDÔºåÂπ∂Âà†Èô§Á¥¢Âºï‰∏≠ÁöÑÊñáÊ°£„ÄÇ
2. ÂΩìÈúÄË¶ÅÂà†Èô§FaissÁ¥¢Âºï‰∏≠ÁöÑÊñáÊ°£Êó∂ÔºåËøêË°åÔºö
```bash
cd src/post_process
python delete_doc_from_index.py --config_file_path delete_configs/delete_config_faiss.json
```
ÂÖ∂‰∏≠Ôºå`src/post_process/delete_configs/delete_config_faiss.json`ÊòØÂØπÂ∫îÈÖçÁΩÆÊñá‰ª∂ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂèÇÊï∞Ôºö
```json
{
     "id_files": ["../run_logs/mis_filter_bleu_nq_webq_pop_tqa_loop_log_contriever_None_total_loop_10_20240206164013/index_add_logs"],
    "model_name": "faiss",
    "index_path": "../../data_v2/indexes",
    "index_name": "contriever_faiss_index",
    "elasticsearch_url": "http://xxx.xxx.xxx.xxx:xxx",
    "delete_log_path": "../run_logs/mis_filter_bleu_nq_webq_pop_tqa_loop_log_contriever_None_total_loop_10_20240206164013/index_add_logs"
}
```
ÂÖ∂‰∏≠Ôºå`id_files`ÊòØÈúÄË¶ÅÂà†Èô§ÁöÑÊñáÊ°£IDÊâÄÂú®Êñá‰ª∂ÁõÆÂΩïÔºå`model_name`ÊòØÁ¥¢ÂºïÊ®°ÂûãÂêçÁß∞Ôºå`index_path`ÊòØÁ¥¢ÂºïÂ≠òÂÇ®Ë∑ØÂæÑÔºå`index_name`ÊòØÁ¥¢ÂºïÂêçÁß∞Ôºå`elasticsearch_url`ÊòØElasticSearchÁöÑurlÔºå`delete_log_path`ÊòØÂà†Èô§ÊñáÊ°£IDËÆ∞ÂΩïÁöÑÂ≠òÂÇ®Ë∑ØÂæÑ„ÄÇ
-->
## More Use Examples
### Using Different LLMs at Different Iteration Stages
By modifying the `GENERATE_MODEL_NAMES` configuration in `src/run_loop.sh`, you can use different LLMs at different stages. For example:
```bash
GENERATE_MODEL_NAMES_F3=(qwen-0.5b-chat qwen-1.8b-chat qwen-4b-chat)
GENERATE_MODEL_NAMES_F7=(qwen-7b-chat llama2-7b-chat baichuan2-7b-chat)
GENERATE_MODEL_NAMES_F10=(gpt-3.5-turbo qwen-14b-chat llama2-13b-chat)
```
Indicates that qwen-0.5b-chat, qwen-1.8b-chat, and qwen-4b-chat are used in the first three rounds of iteration, qwen-7b-chat, llama2-7b-chat, and baichuan2-7b-chat are used in the fourth to seventh rounds of iteration, and gpt-3.5-turbo, qwen-14b-chat, and llama2-13b-chat are used in the eighth to tenth rounds of iteration. 

We can use this to simulate the impact of LLM performance enhancement over time on the RAG system. Remember to provide the corresponding information in `GENERATE_BASE_AND_KEY`, and modify `GENERATE_TASK` to `update_generate`.

### Misinformation Experiment
1. We first use GPT-3.5-Turbo to generate 5 incorrect answers for each query, and then randomly select an incorrect answer, and let all LLMs used in the experiment generate supporting text for it. You can modify the `{dataset}_mis_config_answer_gpt.json` configuration file (used to generate incorrect answers) and `{dataset}_mis_config_passage_{llm}.json` configuration file (used to generate text containing incorrect answers) in the `src/misinfo/mis_config` directory to configure each dataset and generation content path and api information.
2. Run the `src/misinfo/run_gen_misinfo.sh` script to generate Misinformation text for all data and models in one run.
3. Refer to [Running the Code](#running-the-code) steps 4 to 6 to add the generated Misinformation text to the index and obtain the RAG after adding Misinformation data.
4. Refer to [Evaluation](#evaluation) to run the `src/evaluation/run_context_eva.sh` script to evaluate the results of the Misinformation experiment, and it is recommended to use the `"retrieval"`, `"context_answer"`, `"QA_llm_mis"`, and `"QA_llm_right"` evaluation tasks.

### Filtering Experiment
In our experiments, we conducted SELF-BLEU filtering and source filtering of the retrieved contexts separately.
1. **SELF-BLEU filtering**: We calculate the SELF-BLEU value of the top retrieval results to ensure that the context input to the LLM maintains a high degree of similarity (i.e., a low SELF-BLEU value, default is 0.4). To enable this feature, set `FILTER_METHOD_NAME=filter_bleu` in `src/run_loop.sh`.
2. **Source filtering**: We identify text from the LLM in the top retrieval results and exclude it. We use [Hello-SimpleAI/chatgpt-qa-detector-roberta](https://huggingface.co/Hello-SimpleAI/chatgpt-qa-detector-roberta) for identification. Please place the checkpoint in the `ret_model` directory before the experiment starts. To enable this feature, set `FILTER_METHOD_NAME=filter_source` in `src/run_loop.sh`.
3. Refer to [Evaluation](#evaluation) to run the `src/evaluation/run_context_eva.sh` script to evaluate the results of the Filtering experiment, and it is recommended to use the `"retrieval"`, `"QA"`, `"filter_bleu_*"`, and `"filter_source_*"` evaluation tasks.

### Deleting Corresponding Documents from the Index
Since our experiments involve dynamic updates of the index, it is not possible to reconstruct the index from scratch in each simulation. Instead, in each simulation, we record the IDs of the newly added text in the `src/run_logs` directory under the `index_add_logs` directory corresponding to this experiment. After the experiment is over, we delete the corresponding documents from the index using the `src/post_process/delete_doc_from_index.py` script.
1. When you need to delete documents from the BM25 index, run:
```bash
cd src/post_process
python delete_doc_from_index.py --config_file_path delete_configs/delete_config_bm25.json
```
Where `src/post_process/delete_configs/delete_config_bm25.json` is the corresponding configuration file, which contains parameters:
```json
{
    "id_files": ["../run_logs/zero-shot_retrieval_log_low/index_add_logs"],
    "model_name": "bm25",
    "index_path": "../../data_v2/indexes",
    "index_name": "bm25_psgs_index",
    "elasticsearch_url": "http://xxx.xxx.xxx.xxx:xxx",
    "delete_log_path": "../run_logs/zero-shot_retrieval_log_low/index_add_logs"
}
```
Where `id_files` is the directory where the document IDs to be deleted are located, `model_name` is the index model name, `index_path` is the index storage path, `index_name` is the index name, `elasticsearch_url` is the url of ElasticSearch, and `delete_log_path` is the storage path of the document ID record. The ID files of different indexes can be mixed in the same directory, and the script will automatically read the document IDs corresponding to the index in the directory and delete the documents from the index.

2. When you need to delete documents from the Faiss index, run:
```bash
cd src/post_process
python delete_doc_from_index.py --config_file_path delete_configs/delete_config_faiss.json
```
Where `src/post_process/delete_configs/delete_config_faiss.json` is the corresponding configuration file, which contains parameters:
```json
{
     "id_files": ["../run_logs/mis_filter_bleu_nq_webq_pop_tqa_loop_log_contriever_None_total_loop_10_20240206164013/index_add_logs"],
    "model_name": "faiss",
    "index_path": "../../data_v2/indexes",
    "index_name": "contriever_faiss_index",
    "elasticsearch_url": "http://xxx.xxx.xxx.xxx:xxx",
    "delete_log_path": "../run_logs/mis_filter_bleu_nq_webq_pop_tqa_loop_log_contriever_None_total_loop_10_20240206164013/index_add_logs"
}
```
Where `id_files` is the directory where the document IDs to be deleted are located, `model_name` is the index model name, `index_path` is the index storage path, `index_name` is the index name, `elasticsearch_url` is the url of ElasticSearch, and `delete_log_path` is the storage path of the document ID record.
